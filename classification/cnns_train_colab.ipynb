{"cells":[{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"mbpcPlyDPV2y"},"outputs":[],"source":"# not recommended, since it's slow\n!pip install --upgrade tensorflow-gpu"},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"A_WOfc9Xhx9k"},"outputs":[],"source":"import tensorflow as tf\nprint(tf.__version__)\nfrom tensorflow.python.client import device_lib\ndevice_lib.list_local_devices()"},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"WcctKEyftwpG"},"outputs":[],"source":"from google.colab import drive\ndrive.mount('/content/drive')"},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"jVBQ0siXu0d-"},"outputs":[],"source":"!cp drive/My\\ Drive/3DprintedDetection/rendered_transp_512_16_8_1.tar.gz ./\n!sha256sum rendered_transp_512_16_8_1.tar.gz\n!tar xf rendered_transp_512_16_8_1.tar.gz"},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"I-2o_m5aqnuA"},"outputs":[],"source":"!rm -rf images_croped"},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"_mHXc-ddlo36"},"outputs":[],"source":"# train_images/crop_img.py\n# -------------------------------\n\nimport os\nimport cv2\nimport numpy as np\n\n\ndef process(in_path: str, out_path: str, colors=None):\n    img = cv2.imread(in_path, cv2.IMREAD_UNCHANGED)\n    assert img.shape == (512, 512, 4)\n\n    img, mask = crop_img(img)\n    save_imgs(img, mask, out_path, colors)\n\n\ndef crop_img(img, blowout=True):\n    mask = np.zeros((512,) * 2, dtype='uint8')\n    mask[np.where(img[:, :, 3] != 0)] = 255\n\n    if blowout:\n        t_img = cv2.dilate(mask.copy(), np.ones((4, 4), np.uint8), iterations=1)\n\n    _, contours, _ = cv2.findContours(t_img.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n    # Find object with the biggest bounding box\n    mx = (0, 0, 0, 0)      # biggest bounding box so far\n    mx_area = 0\n    org_area = img.shape[0] * img.shape[1]  # area if original image\n\n    for i, cont in enumerate(contours):\n        x, y, w, h = cv2.boundingRect(cont)\n        area = w * h\n        if area > mx_area and area < org_area:\n            mx = x, y, w, h\n            mx_area = area\n\n    # make bounding box quadratic\n    x, y, w, h = mx\n    if w < h:\n        x -= (h - w) // 2\n        w = h\n    elif h < w:\n        y -= (w - h) // 2\n        h = w\n\n    assert x > 0\n    assert y > 0\n    assert x + w < img.shape[1]\n    assert y + h < img.shape[0]\n\n    return img[y:y + h, x:x + w], mask[y:y + h, x:x + w]\n\n\ndef resize_img(img, shape=(256, 256), debug=False):\n    # resize image\n    if debug and (img.shape[0] < shape[0] or img.shape[1] < shape[1]):\n        print(\"Image shape is small: \", img.shape)\n    return cv2.resize(img, shape)\n\n\ndef save_imgs(img, mask, out_path: str, colors=None):\n    if not colors:\n        img = resize_img(img[:, :, :3], shape=(128, 128))\n        cv2.imwrite(out_path, img)\n        return\n\n    img = img[:, :, :3]\n    img = (img * np.random.uniform(low=0.85, high=1.0)).astype('uint8')\n    yellow_off = np.random.randint(0, 17)\n    id0, id1 = np.where(img[:, :, 0] > yellow_off)\n    img[id0, id1, 0] -= yellow_off\n\n    h, w = img.shape[:2]\n    mask = np.dstack((mask, mask, mask))\n    mask_inverse = np.ones(mask.shape, dtype='uint8') * 255 - mask\n\n    for j, col in enumerate(colors):\n        backg = np.zeros((h, w, 3), dtype=np.uint8)\n        backg[:, :] = col\n        max_diffuse = np.random.randint(10, 41, dtype='uint8')\n        backg += np.random.randint(0, max_diffuse,\n                                   size=backg.shape,\n                                   dtype='uint8')\n\n        img_col = cv2.bitwise_and(img, mask)\n        img_col += cv2.bitwise_and(backg, mask_inverse)\n        img_col = resize_img(img_col, shape=(128, 128))\n\n        out_p = \"{}_col{}{}\".format(out_path[:-4], j, out_path[-4:])\n        cv2.imwrite(out_p, img_col)\n\n\nif __name__ == \"__main__\":\n    color_list = [[230, 10, 10], [10, 230, 10], [10, 10, 230], [150, 150, 150]]\n    color_list += [[0, 0, 0], [200, 200, 10], [10, 200, 200], [200, 10, 200]]\n\n    save_path = './images_croped'\n    if not os.path.isdir(save_path):\n        os.mkdir(save_path)\n\n    path = './images'\n    subfolders = os.listdir(path)\n    failed_objects = []\n    for i, folder in enumerate(subfolders):\n        msg = 'Fortschritt - Ordner: {} von {}\\r'.format(i, len(subfolders))\n        print(msg)\n\n        if not os.path.isdir('{}/{}'.format(path, folder)):\n            continue\n\n        if not os.path.isdir('%s/%s' % (save_path, folder)):\n            os.mkdir('%s/%s' % (save_path, folder))\n\n        content = os.listdir('{}/{}'.format(path, folder))\n        for img_name in content:\n            input_path = '%s/%s/%s' % (path, folder, img_name)\n            output_path = '%s/%s/%s' % (save_path, folder, img_name)\n            try:\n                process(input_path, output_path, color_list)\n            except AssertionError:\n                if folder not in failed_objects:\n                    failed_objects.append(folder)\n    print('\\n', failed_objects)"},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"blRBQdNnOtm9"},"outputs":[],"source":"# prediction/calculate_mean_std_batchwise.py\n# -------------------------------\n\nimport os\n\ncnt = 0\nPATH = './images_croped/'\n\nsubfolders = os.listdir(PATH)\nsubfolders = [k for k in subfolders if os.path.isdir('{}{}'.format(PATH, k))]\n\nfor folder in subfolders:\n  nb = len([k for k in os.listdir('{}{}'.format(PATH, folder))])\n  cnt += nb\n\nprint(cnt)"},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"hdZslGpW3xzU"},"outputs":[],"source":"# prediction/calculate_mean_std_batchwise.py\n# -------------------------------\n\nimport numpy as np\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\nTRAIN_PATH = './images_croped/'\nIMG_WIDTH = 128\nBATCH_SIZE = 2048\n\ndatagen = ImageDataGenerator(rescale=1. / 255)\n\ngen = datagen.flow_from_directory(\n    directory=TRAIN_PATH,\n    target_size=(IMG_WIDTH, IMG_WIDTH),\n    color_mode=\"rgb\",\n    batch_size=BATCH_SIZE,\n    class_mode=\"categorical\",\n    shuffle=False\n)\n\nbatches = 88\nassert float(batches) == cnt / BATCH_SIZE\n\nprint(\"Calculate mean\")\n\nmean = np.zeros((IMG_WIDTH, IMG_WIDTH, 3))\nfor i in range(batches):\n    X, _ = gen.next()\n    mean += np.mean(X, axis=0)\n    if i % 10 == 0:\n        print(i)\nmean /= batches\nprint(mean.shape)\n\nprint(\"Calculate std \")\n\nvar = np.zeros((IMG_WIDTH, IMG_WIDTH, 3))\nfor i in range(batches):\n    X, _ = gen.next()\n    var += np.sum((X - mean) ** 2, axis=0)\n    if i % 10 == 0:\n        print(i)\nvar /= (cnt - 1)\nstd = np.sqrt(var)\nprint(std.shape)\n\nnp.save('featurewise_mean', mean)\nnp.save('featurewise_std', std)"},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"a1MGxS9gtlV8"},"outputs":[],"source":"# prediction/calculate_mean_std_batchwise.py\n# -------------------------------\n\n!cp featurewise_std.npy drive/My\\ Drive/ba_classification\n!cp featurewise_mean.npy drive/My\\ Drive/ba_classification"},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"dcvBgpDx8LMw"},"outputs":[],"source":"# copy std and mean from Gdrive\n!cp drive/My\\ Drive/3DprintedDetection/cnn_classification/featurewise_std.npy ./\n!cp drive/My\\ Drive/3DprintedDetection/cnn_classification/featurewise_mean.npy ./"},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"hdyYM9aGb9T-"},"outputs":[],"source":"# prediction/cnn_datagenerator.py\n# -------------------------------\n\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport numpy as np\nimport warnings\n\n\nclass MyImageDataGen(ImageDataGenerator):\n    \"\"\" My own ImageDataGenerator which inherit from the original\n    and applies precalculated std and mean\"\"\"\n\n    def __init__(self, mean=None, std=None, **kwargs):\n        super().__init__(**kwargs)\n        self._mean = mean\n        self._std = std\n\n    def standardize(self, x):\n        if self._mean is not None and self._std is not None:\n            assert x.shape[-3:] == self._mean.shape\n            assert x.shape[-3:] == self._std.shape\n\n        if self.preprocessing_function:\n            x = self.preprocessing_function(x)\n        if self.rescale:\n            x *= self.rescale\n        if self.samplewise_center:\n            x -= np.mean(x, keepdims=True)\n        if self.samplewise_std_normalization:\n            x /= (np.std(x, keepdims=True) + 1e-6)\n\n        if self.featurewise_center:\n            if self._mean is not None:\n                x -= self._mean\n            else:\n                warnings.warn('This MyImageDataGen specifies '\n                              '`featurewise_center`, but the mean isn\\'t given.')\n        if self.featurewise_std_normalization:\n            if self._std is not None:\n                x /= (self._std + 1e-6)\n            else:\n                warnings.warn('This MyImageDataGen specifies '\n                              '`featurewise_std_normalization`, '\n                              'but the std isn\\'t given.')\n        if self.zca_whitening:\n            warnings.warn('This MyImageDataGen specifies '\n                          '`zca_whitening`, but isn\\'t implemented.')\n        return x\n\n\ndef get_train_gen(mean_path='./featurewise_mean.npy', std_path='./featurewise_std.npy'):\n    mean = np.load(mean_path, allow_pickle=True)\n    std = np.load(std_path, allow_pickle=True)\n    kwargs = dict(\n        # set rescaling factor (applied before any other transformation)\n        rescale=1. / 255,\n        # set input mean to 0 over the dataset\n        featurewise_center=True,\n        featurewise_std_normalization=True,\n        # samplewise_center=True,\n        # samplewise_std_normalization=True,\n\n        # randomly rotate images in the range (degrees, 0 to 180)\n        # rotation_range=10,\n        # randomly shift images horizontally (fraction of total width)\n        width_shift_range=0.02,\n        # randomly shift images vertically (fraction of total height)\n        height_shift_range=0.02,\n        # horizontal_flip=True,  # randomly flip images\n        # vertical_flip=True,  # randomly flip images\n    )\n    datagen = MyImageDataGen(mean, std, **kwargs)\n    return datagen\n\n\ndef get_test_gen(mean_path='./featurewise_mean.npy', std_path='./featurewise_std.npy'):\n    mean = np.load(mean_path, allow_pickle=True)\n    std = np.load(std_path, allow_pickle=True)\n    kwargs = dict(\n        # set rescaling factor (applied before any other transformation)\n        rescale=1. / 255,\n        # set input mean to 0 over the dataset\n        featurewise_center=True,\n        featurewise_std_normalization=True,\n        # samplewise_center=True,\n        # samplewise_std_normalization=True,\n    )\n    datagen = MyImageDataGen(mean, std, **kwargs)\n    return datagen"},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"JRMsJnA-gnwI"},"outputs":[],"source":"# Tensorflow 2 needed\n!rm -rf logs\n%load_ext tensorboard"},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"Zj-lTmtYjgjT"},"outputs":[],"source":"%reload_ext tensorboard\n%tensorboard --logdir logs/fit"},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"lY5048OOCZwB"},"outputs":[],"source":"# prediction/cnn_pretrained.py\n# -------------------------------\n\nimport os\nimport matplotlib.pyplot as plt\nimport tensorflow.keras as keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Model\n# from tensorflow.keras.callbacks import TensorBoard\nfrom tensorflow.keras.applications.vgg16 import VGG16\n\n### --- paths --- ###\n\nTRAIN_PATH = './images_croped'\nMODEL_JSON = './cnn_vgg16.json'\nMODEL_WEIGHTS = './cnn_vgg16.hdf5'\n# LOG_DIR = \"logs/fit\"\n\n### --- paths end --- ###\n\n### --- parameters --- ###\n\nTRAIN_CONV = True\n\nimg_width = 128\nnum_classes = 45\nnb_train_samples = 180224\n\nreg = 1e-3\n\nepochs = 8\nbatch_size = 64\nlearning_rate = 1e-4\n\n### --- parameters end --- ###\n\n### --- model --- ###\n\nif os.path.isfile(MODEL_JSON) and os.path.isfile(MODEL_WEIGHTS):\n    print('Load model from file')\n    with open(MODEL_JSON, 'r') as file:\n        json_string = file.readline()\n    model = keras.models.model_from_json(json_string)\n    model.load_weights(MODEL_WEIGHTS)\nelse:\n    print('Build new model')\n    # create the base pre-trained model\n    base_model = VGG16(include_top=False,\n                       input_shape=(img_width, img_width, 3))\n\n    if TRAIN_CONV:\n        # get output Block 4\n        x = base_model.get_layer('block4_conv2').output\n\n        # last convolutional layers\n        x = layers.Conv2D(512, (3, 3),\n                        activation='relu',\n                        padding='same',\n                        name='block4_conv3')(x)\n        x = layers.MaxPooling2D((2, 2), strides=(2, 2),\n                                name='block4_pool')(x)\n    else:\n        # get output Block 4\n        x = base_model.get_layer('block4_pool').output\n\n    # classification block\n    x = layers.Flatten(name='flatten')(x)\n\n    x = layers.Dense(4096, activation='relu', name='fc1',\n                    kernel_regularizer=keras.regularizers.l2(reg))(x)\n    x = layers.Dense(4096, activation='relu', name='fc2',\n                    kernel_regularizer=keras.regularizers.l2(reg))(x)\n\n    predictions = layers.Dense(num_classes, activation='softmax',\n                               name='predictions')(x)\n\n    model = Model(inputs=base_model.input, outputs=predictions)\n\n    for layer in base_model.layers:\n        layer.trainable = False\n\nif TRAIN_CONV:\n    assert not model.get_layer(name='block4_conv2').trainable\n    assert model.get_layer(name='block4_conv3').trainable\nelse:\n    assert not model.get_layer(name='block4_conv3').trainable\n    assert model.get_layer(name='fc1').trainable\n\n### --- end --- ###\n\n### --- training --- ###\n\n# initiate adam optimizer\nopt = keras.optimizers.Adam(lr=learning_rate)\n\nmodel.compile(\n    optimizer=opt,\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n\ndatagen = get_train_gen().flow_from_directory(\n    directory=TRAIN_PATH,\n    target_size=(img_width, img_width),\n    color_mode=\"rgb\",\n    batch_size=batch_size,\n    class_mode=\"categorical\",\n    shuffle=True\n)\n\n# tfboard = TensorBoard(log_dir=LOG_DIR, histogram_freq=1)\n\n# Fit the model on the batches generated by datagen\nhistory = model.fit_generator(\n    generator=datagen,\n    steps_per_epoch=nb_train_samples // batch_size,\n    epochs=epochs,\n    workers=2,\n    verbose=1 #,\n    # callbacks=[tfboard]\n)\n\n### --- end training --- ###\n\n### --- save model --- ###\n\nmodel.summary()\n\njson_string = model.to_json()\nwith open(MODEL_JSON, 'w') as file:\n    file.write(json_string + '\\n')\nmodel.save_weights(MODEL_WEIGHTS)\n\n### --- end save --- ###"},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"5J1ZwbfUFWEW"},"outputs":[],"source":"plt.style.use('ggplot')\nplt.figure(figsize=(12,5))\n\nplt.subplot(1,2,1)\nplt.plot(history.history['acc'])\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\n\nplt.subplot(1,2,2)\nplt.plot(history.history['loss'])\nplt.ylabel('loss')\nplt.xlabel('epoch')\n\nplt.savefig('./cnn_vgg16_train.png')\nplt.show()\nplt.close()"},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"761CuKXpqyOR"},"outputs":[],"source":"!cp cnn_vgg16.json drive/My\\ Drive/ba_classification\n!cp cnn_vgg16.hdf5 drive/My\\ Drive/ba_classification\n!cp cnn_vgg16_train.png drive/My\\ Drive/ba_classification"},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"qseK_sT9ycfi"},"outputs":[],"source":"# prediction/cnn_own.py\n# -------------------------------\n\nimport os\nimport matplotlib.pyplot as plt\nimport tensorflow.keras as keras\nfrom tensorflow.keras.layers import Activation, Conv2D, Dense\nfrom tensorflow.keras.layers import Dropout, Flatten, MaxPooling2D\n\n### --- paths --- ###\n\nTRAIN_PATH = './images_croped'\nMODEL_JSON = './cnn_own.json'\nMODEL_WEIGHTS = './cnn_own.hdf5'\n\n### --- paths end --- ###\n\n### --- parameters --- ###\n\nimg_width = 128\nnum_classes = 45\nnb_train_samples = 180224\n\nreg = 1e-3\n\nepochs = 16\nbatch_size = 64\nlearning_rate = 1e-4\n\n### --- parameters end --- ###\n\n### --- define model --- ###\n\nif os.path.isfile(MODEL_JSON) and os.path.isfile(MODEL_WEIGHTS):\n    print('Load model from file')\n    with open(MODEL_JSON, 'r') as file:\n        json_string = file.readline()\n    model = keras.models.model_from_json(json_string)\n    model.load_weights(MODEL_WEIGHTS)\nelse:\n    print('Build new model')\n    model = keras.Sequential()\n\n    # ConvLayer 1 - Input\n    model.add(Conv2D(\n        32, 3,\n        # strides=(2, 2),\n        padding='same',\n        input_shape=(img_width, img_width, 3),\n        kernel_regularizer=keras.regularizers.l2(reg)\n    ))\n    model.add(Activation('relu'))\n\n    # ConvLayer 2\n    model.add(Conv2D(\n        32, 3,\n        padding='same',\n        kernel_regularizer=keras.regularizers.l2(reg)\n    ))\n    model.add(Activation('relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n\n    # ConvLayer 3\n    model.add(Conv2D(\n        64, 3,\n        padding='same',\n        kernel_regularizer=keras.regularizers.l2(reg)\n    ))\n    model.add(Activation('relu'))\n\n    # ConvLayer 4\n    model.add(Conv2D(\n        64, 3,\n        padding='same',\n        kernel_regularizer=keras.regularizers.l2(reg)\n    ))\n    model.add(Activation('relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n\n    # ConvLayer 5\n    model.add(Conv2D(\n        128, 3,\n        padding='same',\n        kernel_regularizer=keras.regularizers.l2(reg)\n    ))\n    model.add(Activation('relu'))\n\n    # ConvLayer 6\n    model.add(Conv2D(\n        128, 3,\n        padding='same',\n        kernel_regularizer=keras.regularizers.l2(reg)\n    ))\n    model.add(Activation('relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n\n    # Flat\n    model.add(Flatten())\n\n    # Fully Conected Layer 7\n    model.add(Dense(2048, kernel_regularizer=keras.regularizers.l2(reg)))\n    model.add(Activation('relu'))\n    model.add(Dropout(0.5))\n\n    # Fully Conected Layer 8\n\n    model.add(Dense(2048, kernel_regularizer=keras.regularizers.l2(reg)))\n    model.add(Activation('relu'))\n    model.add(Dropout(0.5))\n\n    # Fully Connected Layer 9 - Output\n    model.add(Dense(num_classes, kernel_regularizer=keras.regularizers.l2(reg)))\n    model.add(Activation('softmax'))\n\n### --- end definition --- ###\n\n### --- training --- ###\n\n# initiate adam optimizer\nopt = keras.optimizers.Adam(lr=learning_rate)\n\nmodel.compile(\n    optimizer=opt,\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n\ndatagen = get_train_gen().flow_from_directory(\n    directory=TRAIN_PATH,\n    target_size=(img_width, img_width),\n    color_mode=\"rgb\",\n    batch_size=batch_size,\n    class_mode=\"categorical\",\n    shuffle=True\n)\n\n# Fit the model on the batches generated by datagen\nhistory = model.fit_generator(\n    generator=datagen,\n    steps_per_epoch=nb_train_samples // batch_size,\n    epochs=epochs,\n    workers=2,\n    verbose=1\n)\n\n### --- end training --- ###\n\n### --- save model --- ###\n\nmodel.summary()\n\njson_string = model.to_json()\nwith open(MODEL_JSON, 'w') as file:\n    file.write(json_string + '\\n')\nmodel.save_weights(MODEL_WEIGHTS)\n\n### --- end save --- ###\n\n### --- show learning --- ###"},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"IYSuqgknFcuE"},"outputs":[],"source":"plt.style.use('ggplot')\nplt.figure(figsize=(12,5))\n\nplt.subplot(1,2,1)\nplt.plot(history.history['acc'])\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\n\nplt.subplot(1,2,2)\nplt.plot(history.history['loss'])\nplt.ylabel('loss')\nplt.xlabel('epoch')\n\nplt.savefig('./cnn_own_train.png')\nplt.show()\nplt.close()"},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"LzKJkLNxeHJR"},"outputs":[],"source":"!cp cnn_own.json drive/My\\ Drive/ba_classification\n!cp cnn_own.hdf5 drive/My\\ Drive/ba_classification\n!cp cnn_own_train.png drive/My\\ Drive/ba_classification"},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"HgZQ667FRamG"},"outputs":[],"source":"# rgb vs. hsv visualization\n# taken from https://realpython.com/python-opencv-color-spaces/\n\nimport cv2\nimport numpy as np\nfrom matplotlib import cm\nfrom matplotlib import colors\nfrom google.colab import files\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\nplt.rcParams['figure.figsize'] = [10, 10]\n\nuploaded = files.upload()\nfname = list(uploaded.keys())[0]\n\nnemo = cv2.imread(fname, 1)\nif nemo.shape[0] > 512 or nemo.shape[1] > 512:\n    nemo = cv2.resize(nemo, (512, 512))\n\nnemo = cv2.cvtColor(nemo, cv2.COLOR_BGR2RGB)\nr, g, b = cv2.split(nemo)\nfig = plt.figure()\naxis = fig.add_subplot(1, 1, 1, projection=\"3d\")\n\npixel_colors = nemo.reshape((np.shape(nemo)[0]*np.shape(nemo)[1], 3))\nnorm = colors.Normalize(vmin=-1.,vmax=1.)\nnorm.autoscale(pixel_colors)\npixel_colors = norm(pixel_colors).tolist()\n\naxis.scatter(r.flatten(), g.flatten(), b.flatten(), facecolors=pixel_colors, marker=\".\")\naxis.set_xlabel(\"Red\")\naxis.set_ylabel(\"Green\")\naxis.set_zlabel(\"Blue\")\nplt.title('RGB-Farbraum')\nplt.savefig('rgb_viz.png')\nplt.show()\n\nhsv_nemo = cv2.cvtColor(nemo, cv2.COLOR_RGB2HSV)\n\nh, s, v = cv2.split(hsv_nemo)\nfig = plt.figure()\naxis = fig.add_subplot(1, 1, 1, projection=\"3d\")\n\naxis.scatter(h.flatten(), s.flatten(), v.flatten(), facecolors=pixel_colors, marker=\".\")\naxis.set_xlabel(\"Hue\")\naxis.set_ylabel(\"Saturation\")\naxis.set_zlabel(\"Value\")\nplt.title('HSV-Farbraum')\nplt.savefig('hsv_viz.png')\nplt.show()"},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"t_QDlJin1BkS"},"outputs":[],"source":""}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}